{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cec85-d63c-4b86-ad7b-c6e97f1c1d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain boto3\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47bb0c-cbd7-49e0-92df-01d417231d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168faab2-a433-4196-bb7f-64b4e41c624f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3dc28e-1957-4320-88ac-7f4b3231c94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ~/SageMaker/botocore-1.29.162-py3-none-any.whl\n",
    "!pip install ~/SageMaker/boto3-1.26.162-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bf28c-7f53-4dfe-acb9-17e1015b10de",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "Ensure you have access to Bedrock and the Claude models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728fa95-055c-42dd-941e-4dfef5256b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain import SagemakerEndpoint, LLMChain\n",
    "from langchain import LLMChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import json\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811f373-fdc7-4f7a-af5f-23eac6511f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "#        return response_json[\"vectors\"]\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80373bb-52b3-481f-a5e4-39702610727f",
   "metadata": {},
   "source": [
    "### Create the tools\n",
    "\n",
    "We create the \"tools\" which will be executed by agent. In this example, we create the a tool to generate the fictious TPS reports made famous by the move Office Space. The LLM would parse the input from the user and determine that the tool has to been invoked with the parameter provided by the user. The generateTpsReport tool, returns the query from the user along with asking the LLM to generate a paragraph about the topic the user specified. This is a simplistic example, however this method could easily be modified to lookup a database or invoke and external service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10047f-3803-4216-9c8f-457d3a8dec2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.tools import BaseTool\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "def generateTpsReport(topic):\n",
    "    print(\"\\nthe tps report tool executed\\n\")\n",
    "    return \"<function_result>here is a report about \" + topic + \". Write 5 sentences in a paragraph about \" + topic + \" as a official status report </function_result>\"\n",
    "\n",
    "\n",
    "tps_report_tool = Tool(\n",
    "    name=\"generateTpsReport\",\n",
    "    func = generateTpsReport,\n",
    "    description=\"a tool to generate tps reports\"\n",
    ")\n",
    "\n",
    "tools = [tps_report_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7eabdb-f87e-4aaa-8737-d9f2de3b5ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The ReAct prompt\n",
    "\n",
    "A prompt template is created to feed the LLM and it includes an example for the LLM to understand how process the inputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de4a04-ea58-406f-bbf0-04a45681eca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claude_template = \"\"\"Human: You are a research assistant AI that has been equipped with the following function(s) to help you answer a <question>. Your goal is to answer the user's question to the best of your ability, using the function(s) to gather more information if necessary to better answer the question. The result of a function call will be added to the conversation history as an observation.\n",
    "\n",
    "\"Here are the only function(s) I have provided you with:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Human: \"To call a function, output <function_call>insert specific function</function_call>. You will receive a <function_result> in response to your call that contains information that you can use to better answer the question. You should not geneate a function_result, it should be provided to you\n",
    "\n",
    "Here is an example of how you would correctly answer a question using a <function_call> and the corresponding <function_result>. Notice that you are free to think before deciding to make a <function_call> in the <scratchpad>:\n",
    "\n",
    "<example>\n",
    "<functions>\n",
    "<function>\n",
    "<function_name>generateTpsReport</function_name>\n",
    "<function_description>Generates a TPS report</function_description>\n",
    "<required_argument>report_subject(string): the topic the report needs to be generated about </required_argument>\n",
    "<returns>string: A document containting the report.</returns>\n",
    "<raises>ValueError: if a valid topic is not provided </raises>\n",
    "<example_call>generate_tps_reports(topic=\"\"red_stapler\"\")</example_call>\n",
    "</function>\n",
    "</functions>\n",
    "\n",
    "<question>Generate a TPS report about staplers?</question>\n",
    "\n",
    "<scratchpad>I do not have access to generate random numbers so I should use a function to gather more information to answer this question. I have been equipped with the function get_random_number that gets a random number  so I should use that to gather more information.\n",
    "\n",
    "I have double checked and made sure that I have been provided the generate_tps_reports function.\n",
    "</scratchpad>\n",
    "\n",
    "<function_call>generateTpsReport(topic=\"\"stapler\"\")</function_call>\n",
    "\n",
    "<function_result>Milton has a red stapler</function_result>\n",
    "\n",
    "<answer>Milton has a red stapler</answer>\n",
    "</example>\"\n",
    "\n",
    "<question>{input}</question>\n",
    "\n",
    "{agent_scratchpad}\n",
    "\n",
    "Assistant:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444611a2-1d9e-4bd6-b66d-d7d7c15dd4f9",
   "metadata": {},
   "source": [
    "### The Prompt Template\n",
    "\n",
    "A custom prompt template is defined which will insert the actual data in the location of the macros in the prompt ```{input}``` and ```{tool}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae89d4-438a-404d-bb5e-1c2ce4f5838b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    ############## NEW ######################\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\n\\n{observation}\"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        ############## NEW ######################\n",
    "        #tools = self.tools_getter(kwargs[\"input\"])\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        #kwargs[\"tools\"] = \"\\n\".join(\n",
    "        #    [f\"{tool.name}: {tool.description}\" for tool in tools]\n",
    "        #)\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\n",
    "        #print(\"the kwargs in the format function before returning is\")\n",
    "        #print(kwargs)\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "claude_prompt = CustomPromptTemplate(\n",
    "    template=claude_template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30be9f9-c2fa-432b-9a29-d39c5682ee69",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "\n",
    "The agent invokes the Output parser once it receives a response from the LLM. The Output parser is responsible for examining the output from the LLM and determining the tool to use and the parameters to send to the tool. This class determines the next action an agent should take, which could either be AgentFinish or AgentAction. \n",
    "\n",
    "In the prompt, we have instructed the LLM to answer with the ```<answer>``` XML tag once it has all the information for the answer, hence we look for the ```<answer>``` tag in the LLM output and if present, we extract the answer, and wrap it in an AgentFinish object and return it. When the agent receives the AgentFinish object it stops the execution of the agent and provides the answer to the end-user\n",
    "\n",
    "If the prompt has ```<function_call>``` we return an object of AgentAction that contains the name of the function to invoke and the parameters to pass to the function. The agent will use the information from this object to invoke the tool and pass the response back to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ec5dc-9603-4ee8-b832-c92f83196e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "\n",
    "        # Check if agent should finish\n",
    "        if \"<answer>\" in llm_output:\n",
    "            #print(\"Agent is finished\")\n",
    "            root = ET.fromstring(llm_output)\n",
    "            output = root.text\n",
    "\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "               return_values={\"output\": output},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        linesInOutput = llm_output.splitlines()\n",
    "        #print(len(linesInOutput))\n",
    "        \n",
    "        for line in linesInOutput:\n",
    "            if \"function_call\" in line:\n",
    "                root = ET.fromstring(line)\n",
    "                function = root.text\n",
    "                \n",
    "                nameOfFunctionToInvoke = function.split('(')[0]\n",
    "                parameter = function.split('=')[1][1:-2]\n",
    "        \n",
    "        #print(\"tool executed\")\n",
    "\n",
    "        return AgentAction(\n",
    "            #tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
    "            tool=nameOfFunctionToInvoke, tool_input=parameter, log=llm_output\n",
    "        )\n",
    "    \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be9f0c-eedd-47f0-ad91-7c5af245b210",
   "metadata": {},
   "source": [
    "### Content Handlers\n",
    "\n",
    "Content Handlers are defined to convert the input to the input expected by the model and to extract the response from the JSON returned by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215b923-c2e1-40d4-be48-07f1f2d57136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import json\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "#        return response_json[\"vectors\"]\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b31f4-72fd-4a25-af69-3871ecc6e8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "#        return response_json[\"vectors\"]\n",
    "        return response_json[0][\"generation\"]\n",
    "\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "llama_content_handler = LlamaContentHandler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f7a1e-6118-4d33-a0d2-3846e02ce948",
   "metadata": {},
   "source": [
    "### Define the LLMs\n",
    "\n",
    "We define the LLMs that we want to use for predictions. Ensure that you have access to Bedrock/Claude before you run this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddb06c-484c-4a26-9822-25ddedb65768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelArgs = {'max_tokens_to_sample': 250, 'temperature':0, \"top_k\":5,\"top_p\": 0.9,\"stop_sequences\":[]}\n",
    "#modelArgs = {'max_tokens_to_sample': int(maxTokensToSample), 'temperature':float(temp), \"top_k\":int(topK),\"top_p\": float(topP),\"stop_sequences\":[]}\n",
    "llm2 = Bedrock(model_id=\"anthropic.claude-v2\",model_kwargs=modelArgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bed45-36ed-460b-a461-e0ae942b7b72",
   "metadata": {},
   "source": [
    "### Define the LLM Chain/Agent/AgentExecutor\n",
    "\n",
    "The LLM chain is defined and we provide the LLM that we want to use along with the PromptTemplate that we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03328d35-fc62-451b-a833-1fbfcf1721e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm2, prompt=claude_prompt)\n",
    "\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    #llm_chain=llm_chain,\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\n\\nHuman:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1506337-79b8-4dd9-9491-42a958f65b82",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Now that we have defined the tools, the prompts and the agent, we can perform an inference. When you run the block below you should see an output like \n",
    "\n",
    "```\n",
    " <scratchpad>\n",
    "I do not have enough information to generate a full TPS report for this question. However, I have been provided with the generateTpsReport function that can help me gather more details to answer this properly. I will call that function to get more information.\n",
    "</scratchpad>\n",
    "\n",
    "<function_call>generateTpsReport(topic=\"software engineer fixing 5 bugs\")</function_call>\n",
    "the tps report tool executed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Human:<function_result>here is a report about software engineer fixing 5 bugs. Write 5 sentences in a paragraph about software engineer fixing 5 bugs as a official status report </function_result>\n",
    " <answer>\n",
    "Here is a TPS report about a software engineer fixing 5 bugs in a day:\n",
    "\n",
    "The software engineer started the day by reviewing the bug tracking system and prioritizing the top 5 critical bugs reported by customers. The first bug was a crash that occurred when exporting data, which the engineer was able to quickly reproduce and fix in about an hour. The second and third bugs were related to formatting issues in the UI, which required some tweaks to the CSS styling to resolve. The fourth bug was a tricky logical error in the validation logic, needing careful debugging of the code flow. The fifth bug was a performance issue that required some optimization of the database queries. Through persistence and diligence, the software engineer was able to fix all 5 high priority bugs by the end of the day, improving the product and customer experience.\n",
    "</answer>\n",
    "```\n",
    "A debug statement added to the tool which is printed to the console ```the tps report tool executed``` confirms that the tool was invoked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19553cd-0c64-485b-ad7e-ad6f30e51136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.debug = False\n",
    "agent_executor.run(\"run TPS report for finishing all the timesheets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335b9a3-1345-4e95-b54d-fc9fb4cae7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
